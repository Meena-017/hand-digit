{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc8b9ICBSjL1"
      },
      "source": [
        "Importing the TensorFlow library and loads the MNIST dataset, which consists of 70,000 grayscale images of handwritten digits (0–9), each sized 28x28 pixels. The dataset is automatically split into a training set and a test set, where `x_train` and `y_train` contain the images and labels for training (60,000 samples), and `x_test` and `y_test` contain the images and labels for testing (10,000 samples). The printed shapes of these arrays help verify that the data has been loaded correctly and show the structure of the input and output data. This dataset is commonly used for training and evaluating models in image classification tasks, especially for beginners in deep learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV8S2DyPLZXB",
        "outputId": "21c76621-7f4a-4753-c7bc-328acdefddc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "x_train shape: (60000, 28, 28)\n",
            "y_train shape: (60000,)\n",
            "x_test shape: (10000, 28, 28)\n",
            "y_test shape: (10000,)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESZVAgV8TFpV"
      },
      "source": [
        "This code installs the required libraries (scikit-image, OpenCV, and TensorFlow) and then extracts meaningful features from the MNIST dataset using various image processing techniques. It uses the Histogram of Oriented Gradients (HOG) feature descriptor to capture edge and texture information from the images. Additionally, it computes image characteristics such as the mean and variance of pixel values, histogram equalization for contrast enhancement, and edge detection using the Canny algorithm. These features are then compiled into feature vectors for each image in both the training and test sets. The resulting features are saved into CSV files (`mnist_train_features.csv` and `mnist_test_features.csv`), which can be used for further analysis or machine learning model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srEi96oCPGAX",
        "outputId": "cf97c942-7e52-4d4f-f2b2-a93b721bcee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.14.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.1.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Feature extraction and CSV creation complete.\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-image opencv-python tensorflow\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage.feature import hog\n",
        "from skimage import exposure\n",
        "import cv2\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "def extract_features(images):\n",
        "    all_features = []\n",
        "    for img in images:\n",
        "        image_features = []\n",
        "        fd, hog_image = hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
        "                            cells_per_block=(2, 2), visualize=True, channel_axis=None)\n",
        "        image_features.extend(fd)\n",
        "\n",
        "        equ = exposure.equalize_hist(img)\n",
        "        image_features.append(np.mean(equ))\n",
        "\n",
        "        edges = cv2.Canny(img, 100, 200)\n",
        "        image_features.append(np.sum(edges))\n",
        "\n",
        "        image_features.append(np.mean(img))\n",
        "\n",
        "        image_features.append(np.var(img))\n",
        "\n",
        "        all_features.append(image_features)\n",
        "\n",
        "    return np.array(all_features)\n",
        "\n",
        "\n",
        "train_features = extract_features(x_train)\n",
        "\n",
        "test_features = extract_features(x_test)\n",
        "\n",
        "\n",
        "train_df = pd.DataFrame(train_features)\n",
        "train_df['label'] = y_train\n",
        "\n",
        "test_df = pd.DataFrame(test_features)\n",
        "test_df['label'] = y_test\n",
        "\n",
        "train_df.to_csv('mnist_train_features.csv', index=False)\n",
        "test_df.to_csv('mnist_test_features.csv', index=False)\n",
        "\n",
        "print(\"Feature extraction and CSV creation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59l02SVZTSQO"
      },
      "source": [
        "This code attempts to load the previously saved CSV files containing the MNIST dataset features using the `pandas` library. It first tries to read the `mnist_train_features.csv` and `mnist_test_features.csv` files. If successful, it prints the first few rows of each dataset to display a snapshot of the training and testing data. The code also includes error handling to catch various issues: if the files are not found, it will display a `FileNotFoundError` message; if the files are empty, it will display an `EmptyDataError` message; and if there's a problem with parsing the files, it will raise a `ParserError`. Any other unexpected errors are captured by a general exception handler, ensuring that the program doesn't crash unexpectedly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uS9C7fjOzXL",
        "outputId": "44df1744-f4ef-4882-bf83-be72e1ebd083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "          0    1         2    3         4         5    6         7    8  \\\n",
            "0  0.114675  0.0  0.022903  0.0  0.000000  0.000000  0.0  0.000000  0.0   \n",
            "1  0.000000  0.0  0.000000  0.0  0.000000  0.000000  0.0  0.000000  0.0   \n",
            "2  0.177914  0.0  0.155180  0.0  0.058893  0.071822  0.0  0.061913  0.0   \n",
            "3  0.000000  0.0  0.000000  0.0  0.000000  0.000000  0.0  0.000000  0.0   \n",
            "4  0.000000  0.0  0.000000  0.0  0.000000  0.000000  0.0  0.000000  0.0   \n",
            "\n",
            "          9  ...       139  140       141       142       143       144  \\\n",
            "0  0.000000  ...  0.064006  0.0  0.181533  0.000000  0.179817  0.813206   \n",
            "1  0.003872  ...  0.054652  0.0  0.000000  0.010408  0.013600  0.802690   \n",
            "2  0.000000  ...  0.000000  0.0  0.000000  0.000000  0.357638  0.859121   \n",
            "3  0.000000  ...  0.000000  0.0  0.000000  0.000000  0.000000  0.885738   \n",
            "4  0.000000  ...  0.000000  0.0  0.000000  0.073144  0.050103  0.836489   \n",
            "\n",
            "       145        146          147  label  \n",
            "0  24735.0  35.108418  6343.935950      5  \n",
            "1  27285.0  39.661990  7037.055392      0  \n",
            "2  21420.0  24.799745  4300.703520      4  \n",
            "3  15300.0  21.855867  4366.419277      1  \n",
            "4  20655.0  29.609694  5531.092559      9  \n",
            "\n",
            "[5 rows x 149 columns]\n",
            "\n",
            "Testing Data:\n",
            "          0    1         2         3         4    5    6    7         8  \\\n",
            "0  0.046004  0.0  0.158264  0.145045  0.147322  0.0  0.0  0.0  0.000000   \n",
            "1  0.166356  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.075152   \n",
            "2  0.000000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.000000   \n",
            "3  0.000000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.000000   \n",
            "4  0.000000  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.000000   \n",
            "\n",
            "          9  ...       139       140       141       142       143       144  \\\n",
            "0  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.863724   \n",
            "1  0.076573  ...  0.349542  0.051727  0.000000  0.000000  0.000000  0.814924   \n",
            "2  0.466095  ...  0.000000  0.000000  0.000000  0.000000  0.000000  0.921865   \n",
            "3  0.215484  ...  0.032593  0.000000  0.000000  0.000000  0.012571  0.788740   \n",
            "4  0.332529  ...  0.000000  0.000000  0.008672  0.060945  0.218709  0.859290   \n",
            "\n",
            "       145        146          147  label  \n",
            "0  19380.0  23.538265  4353.409250      7  \n",
            "1  25500.0  36.798469  6705.255304      2  \n",
            "2  10965.0  12.590561  2387.943329      1  \n",
            "3  24735.0  47.211735  8538.503638      0  \n",
            "4  19635.0  24.536990  4476.746081      4  \n",
            "\n",
            "[5 rows x 149 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    train_df = pd.read_csv('mnist_train_features.csv')\n",
        "    test_df = pd.read_csv('mnist_test_features.csv')\n",
        "\n",
        "    print(\"Training Data:\")\n",
        "    print(train_df.head())\n",
        "\n",
        "    print(\"\\nTesting Data:\")\n",
        "    print(test_df.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: One or both CSV files not found. Please make sure 'mnist_train_features.csv' and 'mnist_test_features.csv' exist in the current directory.\")\n",
        "except pd.errors.EmptyDataError:\n",
        "    print(\"Error: One or both CSV files are empty.\")\n",
        "except pd.errors.ParserError:\n",
        "    print(\"Error: Could not parse the CSV file(s). Check the file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQSyM1RuTgyN"
      },
      "source": [
        "This code performs model training and evaluation using various machine learning algorithms on the MNIST dataset features stored in CSV files. After loading the training and testing data, it separates the features (`X_train` and `X_test`) from the labels (`y_train` and `y_test`). The script then defines a set of machine learning models, including K-Nearest Neighbors (KNN), Random Forest, Support Vector Machine (SVM), Decision Tree, and Logistic Regression. Each model is trained on the training data, and its performance is evaluated on the test data using multiple metrics: accuracy, precision, and F1 score. The code also prints a detailed classification report for each model, which includes precision, recall, and F1 score per class. Finally, it prints a summary comparison of the models' performance. Error handling is implemented to catch issues such as missing files or other unexpected errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3fH-7zNPM2t",
        "outputId": "f8a2efc0-5a2e-4892-c5b6-dd8ce190bc8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training KNN...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.52      0.37       980\n",
            "           1       0.80      0.90      0.85      1135\n",
            "           2       0.15      0.22      0.18      1032\n",
            "           3       0.14      0.14      0.14      1010\n",
            "           4       0.20      0.20      0.20       982\n",
            "           5       0.12      0.08      0.10       892\n",
            "           6       0.13      0.09      0.11       958\n",
            "           7       0.25      0.24      0.24      1028\n",
            "           8       0.12      0.07      0.09       974\n",
            "           9       0.13      0.08      0.10      1009\n",
            "\n",
            "    accuracy                           0.26     10000\n",
            "   macro avg       0.23      0.25      0.24     10000\n",
            "weighted avg       0.24      0.26      0.25     10000\n",
            "\n",
            "Training Random Forest...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97       980\n",
            "           1       0.98      0.99      0.99      1135\n",
            "           2       0.95      0.97      0.96      1032\n",
            "           3       0.93      0.94      0.94      1010\n",
            "           4       0.95      0.96      0.95       982\n",
            "           5       0.96      0.95      0.95       892\n",
            "           6       0.96      0.97      0.97       958\n",
            "           7       0.96      0.91      0.93      1028\n",
            "           8       0.95      0.94      0.94       974\n",
            "           9       0.93      0.93      0.93      1009\n",
            "\n",
            "    accuracy                           0.95     10000\n",
            "   macro avg       0.95      0.95      0.95     10000\n",
            "weighted avg       0.95      0.95      0.95     10000\n",
            "\n",
            "Training SVM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.73      0.44       980\n",
            "           1       0.88      0.89      0.89      1135\n",
            "           2       0.21      0.17      0.19      1032\n",
            "           3       0.15      0.18      0.16      1010\n",
            "           4       0.18      0.44      0.25       982\n",
            "           5       0.10      0.00      0.00       892\n",
            "           6       0.36      0.00      0.01       958\n",
            "           7       0.30      0.52      0.38      1028\n",
            "           8       0.00      0.00      0.00       974\n",
            "           9       0.13      0.03      0.05      1009\n",
            "\n",
            "    accuracy                           0.31     10000\n",
            "   macro avg       0.26      0.30      0.24     10000\n",
            "weighted avg       0.27      0.31      0.25     10000\n",
            "\n",
            "Training Decision Tree...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.89      0.88       980\n",
            "           1       0.95      0.97      0.96      1135\n",
            "           2       0.81      0.83      0.82      1032\n",
            "           3       0.77      0.77      0.77      1010\n",
            "           4       0.85      0.82      0.84       982\n",
            "           5       0.83      0.83      0.83       892\n",
            "           6       0.88      0.86      0.87       958\n",
            "           7       0.84      0.83      0.83      1028\n",
            "           8       0.78      0.76      0.77       974\n",
            "           9       0.80      0.80      0.80      1009\n",
            "\n",
            "    accuracy                           0.84     10000\n",
            "   macro avg       0.84      0.84      0.84     10000\n",
            "weighted avg       0.84      0.84      0.84     10000\n",
            "\n",
            "Training Logistic Regression...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.82      0.79       980\n",
            "           1       0.96      0.95      0.95      1135\n",
            "           2       0.84      0.75      0.79      1032\n",
            "           3       0.76      0.70      0.73      1010\n",
            "           4       0.77      0.76      0.76       982\n",
            "           5       0.84      0.81      0.83       892\n",
            "           6       0.86      0.89      0.87       958\n",
            "           7       0.75      0.76      0.76      1028\n",
            "           8       0.70      0.76      0.73       974\n",
            "           9       0.63      0.67      0.64      1009\n",
            "\n",
            "    accuracy                           0.79     10000\n",
            "   macro avg       0.79      0.79      0.79     10000\n",
            "weighted avg       0.79      0.79      0.79     10000\n",
            "\n",
            "\n",
            "Comparison of Model Performance:\n",
            "KNN:\n",
            "  Accuracy: 0.2648\n",
            "  Precision: 0.2418\n",
            "  F1 Score: 0.2473\n",
            "Random Forest:\n",
            "  Accuracy: 0.9535\n",
            "  Precision: 0.9536\n",
            "  F1 Score: 0.9534\n",
            "SVM:\n",
            "  Accuracy: 0.3094\n",
            "  Precision: 0.2721\n",
            "  F1 Score: 0.2503\n",
            "Decision Tree:\n",
            "  Accuracy: 0.8392\n",
            "  Precision: 0.8389\n",
            "  F1 Score: 0.8389\n",
            "Logistic Regression:\n",
            "  Accuracy: 0.7869\n",
            "  Precision: 0.7895\n",
            "  F1 Score: 0.7875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, classification_report\n",
        "\n",
        "try:\n",
        "    train_df = pd.read_csv('mnist_train_features.csv')\n",
        "    test_df = pd.read_csv('mnist_test_features.csv')\n",
        "\n",
        "    X_train = train_df.drop('label', axis=1)\n",
        "    y_train = train_df['label']\n",
        "    X_test = test_df.drop('label', axis=1)\n",
        "    y_test = test_df['label']\n",
        "\n",
        "    models = {\n",
        "        \"KNN\": KNeighborsClassifier(),\n",
        "        \"Random Forest\": RandomForestClassifier(),\n",
        "        \"SVM\": SVC(),\n",
        "        \"Decision Tree\": DecisionTreeClassifier(),\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=1000)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        results[name] = {\n",
        "            \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "            \"Precision\": precision_score(y_test, y_pred, average='weighted'),\n",
        "            \"F1 Score\": f1_score(y_test, y_pred, average='weighted')\n",
        "        }\n",
        "        print(classification_report(y_test,y_pred))\n",
        "\n",
        "    print(\"\\nComparison of Model Performance:\")\n",
        "    for name, metrics in results.items():\n",
        "        print(f\"{name}:\")\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"  {metric}: {value:.4f}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: One or both CSV files not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVRmfTnmTm71"
      },
      "source": [
        "Installing Gradio to deploy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfljs4N_ngKX",
        "collapsed": true,
        "outputId": "bd6d097e-e881-407f-b579-a2b0fe546322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.25.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.25.2-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.25.2 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.5 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.1\n"
          ]
        }
      ],
      "source": [
        "    !pip install gradio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7uXzfgBTwyJ"
      },
      "source": [
        "This code creates a Gradio interface for a handwritten digit recognition system. It allows users to upload an image of a handwritten digit, select a machine learning model, and receive a prediction of the digit. The `predict_image` function first processes the uploaded image: it resizes the image to 28x28 pixels, converts it to grayscale, and extracts relevant features using the `extract_features` function. Depending on the model selected by the user (SVM, KNN, Random Forest, Decision Tree, or Logistic Regression), it uses the corresponding trained model to predict the digit and returns the result as a string. If there’s an error during prediction, it provides an error message. The Gradio interface is set up with an image input, a radio button for model selection, and a text output to display the prediction. The app is launched with debugging enabled to aid troubleshooting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "5p7dxtI_q2p2",
        "outputId": "eb1bb5ef-c8c9-4051-a18a-836d3523be99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://5b2bc619b53241fba6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5b2bc619b53241fba6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://5b2bc619b53241fba6.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def predict_image(image, model_choice):\n",
        "    try:\n",
        "        image = cv2.resize(image, (28, 28))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "        image_features = extract_features(np.array([image]))\n",
        "\n",
        "        if model_choice == \"SVM\":\n",
        "            prediction = svm_model.predict(image_features)[0]\n",
        "        elif model_choice == \"KNN\":\n",
        "            prediction = models[\"KNN\"].predict(image_features)[0]\n",
        "        elif model_choice == \"Random Forest\":\n",
        "            prediction = models[\"Random Forest\"].predict(image_features)[0]\n",
        "        elif model_choice == \"Decision Tree\":\n",
        "            prediction = models[\"Decision Tree\"].predict(image_features)[0]\n",
        "        elif model_choice == \"Logistic Regression\":\n",
        "            prediction = models[\"Logistic Regression\"].predict(image_features)[0]\n",
        "        else:\n",
        "            prediction = \"Invalid model selected\"\n",
        "\n",
        "        return str(prediction)\n",
        "    except Exception as e:\n",
        "        return f\"Error during prediction: {e}\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict_image,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"numpy\"),\n",
        "        gr.Radio([\"SVM\", \"KNN\", \"Random Forest\", \"Decision Tree\", \"Logistic Regression\"], label=\"Choose a model\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Handwritten Digit Recognition\",\n",
        "    description=\"Upload an image of a handwritten digit (0-9) and select a model to predict the digit.\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg_gWR4-WsF6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}